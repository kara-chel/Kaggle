{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kapbi4/pytorch-resnet\n"
<<<<<<< HEAD
    "\n"
=======
>>>>>>> 43fa85c2eb60919471b11a1048c6d63c71a38d97
    "Всем доброго времени суток!\n",
    "\n",
    "Оффтопик: Только начинаю погружаться в DL, поэтому прошу сильно не пинать, а укажите на ошибки.\n",
    "\n",
    "<strike>Бывает такое что модель уже имеется и натренирована, надо подогнать входные данные под эту модель.</strike>\n",
    "<strike>Ни для кого не секрет что можно заменять любые слои в натренированной модель, но эти слои нужно будет треноровать заново.</strike>\n",
    "<strike>Поэтому минимизируем изменения и будем адаптировать входные данные использюя натренированую модель ResNet18, этой модели более чем достаточно для таких данных.</strike>\n",
    "\n",
    "Итоговый результат: 0.99657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import albumentations\n",
    "from albumentations.pytorch import ToTensorV2 as AT\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Прогрессбар\n",
    "from tqdm import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "#from tqdm.auto import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# разделить на тренировочные и тестовые(валидационные) данные\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "EPOCH_COUNT = 150 # модель натренируется за долго до этой эпохи\n",
    "patience = 10\n",
    "batch_size = 64\n",
    "n_class = 10\n",
    "train_filename = os.path.join('/', 'kaggle', 'input', 'digit-recognizer', 'train.csv')\n",
    "model_filename = ''\n",
    "# Раскомментировать если надо подгрузить свою модель\n",
    "# model_filename = os.path.join('/', 'kaggle', 'working', 'model.pth')\n",
    "\n",
    "# Скорость обучения\n",
    "LR = 3e-4\n",
    "\n",
    "# Параметры оптимизатора Adam\n",
    "#betas = (0.9, 0.999)\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "# Используем GPU?\n",
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and GPU else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(imgs, lbls, epoh='', batch=''):\n",
    "    ''' Визаулизация данных после аугментаций '''\n",
    "    fig = plt.figure(figsize=(15, 16))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = np.rollaxis(img.numpy(), 0, 3)\n",
    "        img = np.uint8(img)\n",
    "        ax = fig.add_subplot(int(math.sqrt(len(imgs)))+1, int(math.sqrt(len(imgs))), i+1)\n",
    "        ax.set_title(str(lbls.numpy()[i]))\n",
    "        ax.imshow(img, cmap = cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
   ],
   "source": [
    "# загружаем данные\n",
    "data = pd.read_csv(train_filename)\n",
    "\n",
    "# получаем метки\n",
    "Y_train = data['label']\n",
    "\n",
    "# Проверяем число разновидностей меток\n",
    "labels_count = np.unique(Y_train).shape[0]\n",
    "\n",
    "# получаем изобрадения и переводим во float\n",
    "X_train = data.drop(axis=1, columns='label')\n",
    "del data\n",
    "\n",
    "# визуализировать данные\n",
    "# показать случайные 4*8 изображения\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for j in range(4*8):\n",
    "    ax = fig.add_subplot(4, 8, j+1)\n",
    "    i = random.randrange(0,len(X_train))\n",
    "    ax.set_title(str(Y_train[i]))\n",
    "    ax.imshow(X_train.values[i].reshape(28,28), cmap = cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделяем данные на тренировачные и валидационные\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    test_size = 0.1, # размер валидационных данных\n",
    "    random_state = None, # np.random\n",
    "    #random_state = 42,\n",
    "    shuffle = True)\n",
    "\n",
    "# переводим в тензор\n",
    "featuresTrain = torch.from_numpy(np.array(features_train, \n",
    "                                          dtype=np.float32))\n",
    "targetsTrain = torch.from_numpy(np.array(targets_train))\n",
    "featuresTest = torch.from_numpy(np.array(features_test, \n",
    "                                         dtype=np.float32))\n",
    "targetsTest = torch.from_numpy(np.array(targets_test))\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = x.numpy()\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_GRAY2BGR) # Преобразование из 224,224,1 в 224,224,3\n",
    "            x = self.transform(image=x)\n",
    "            x = x['image']\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "\n",
    "# аугментация изображений\n",
    "train_transform = albumentations.Compose([\n",
    "    # Изменяем размер с 28,28 в 224,224\n",
    "    albumentations.Resize(224, 224), \n",
    "    # Вырезаем части изображения\n",
    "    albumentations.Cutout(num_holes=20, \n",
    "                          max_h_size=20, \n",
    "                          max_w_size=20, \n",
    "                          fill_value=0,\n",
    "                          always_apply=True,\n",
    "                          p=1), \n",
    "    # сдигаем, увеличиваем, поварачиваем\n",
    "    albumentations.ShiftScaleRotate(shift_limit=0.2,\n",
    "                                    scale_limit=0.1,\n",
    "                                    rotate_limit=(-20,20),\n",
    "                                    interpolation=1,\n",
    "                                    border_mode=cv2.BORDER_CONSTANT, \n",
    "                                    value=None,\n",
    "                                    mask_value=None,\n",
    "                                    always_apply=False,\n",
    "                                    p=0.5),\n",
    "    # albumentations.OpticalDistortion(p=1),\n",
    "    # добавляем шума\n",
    "    # albumentations.MultiplicativeNoise(multiplier=(0.99, 1.01), \n",
    "    #                                   per_channel=False, \n",
    "    #                                   #elementwise=True, \n",
    "    #                                   elementwise=False, \n",
    "    #                                   always_apply=False, \n",
    "    #                                   p=0.5), \n",
    "    # добавляем шума\n",
    "    # albumentations.GaussNoise(var_limit=(0.1, 1.0), \n",
    "    #                          mean=0, \n",
    "    #                          always_apply=False, \n",
    "    #                          p=0.5),\n",
    "    # добавляем шума\n",
    "    # albumentations.ISONoise(color_shift=(0.01, 0.05), \n",
    "    #                        intensity=(0.1, 0.5), \n",
    "    #                        always_apply=False, p=0.5),\n",
    "    # добавляем размытие\n",
    "    # albumentations.Blur(blur_limit=(7, 7), p=1)\n",
    "    # albumentations.Normalize(),\n",
    "    # Переделывает из 224,224,3 в 3,224,224\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "    ])\n",
    "\n",
    "test_transform = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    #albumentations.Normalize(),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "    ])\n",
    "\n",
    "train = CustomTensorDataset(tensors=(featuresTrain, targetsTrain),\n",
    "                           transform=train_transform)\n",
    "test = CustomTensorDataset(tensors=(featuresTest, targetsTest), \n",
    "                           transform=test_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, \n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получить текущий lr\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e578677511649eeb8e2956d15d447d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0-train:   0%|                                    | 0/591 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-0-train: 100%|██████████████████████████| 591/591 [03:26<00:00,  2.86it/s]\n",
      "epoch-1-train:   0%|                                    | 0/591 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 0 lr: 0.0003\n",
      "train_average_losses_all_epoch:  0.20926421086290453\n",
      "train_losses_all_epoch:          0.20926421086290453\n",
      "train_losses_per_epoch:              0.09411431768950068\n",
      "val_average_losses_all_epoch:  0.06393259304437869\n",
      "val_losses_all_epoch:          0.06393259304437869\n",
      "val_losses_per_epoch:              0.061402709053998646\n",
      "acc_all_epoch:  98.04761904761905\n",
      "acc:            98.04761904761905\n",
      "Validation loss decreased (inf --> 0.061403).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-6-train: 100%|██████████████████████████| 591/591 [03:24<00:00,  2.89it/s]\n",
      "epoch-7-train:   0%|                                    | 0/591 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 6 lr: 0.0003\n",
      "train_average_losses_all_epoch:  0.0997364601137801\n",
      "train_losses_all_epoch:          0.05620917721056124\n",
      "train_losses_per_epoch:              0.0231062045475301\n",
      "val_average_losses_all_epoch:  0.04019390635870902\n",
      "val_losses_all_epoch:          0.029990468452130582\n",
      "val_losses_per_epoch:              0.012897569562703597\n",
      "acc_all_epoch:  99.21088435374148\n",
      "acc:            99.71428571428571\n",
      "Validation loss decreased (0.014417 --> 0.012898).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch-17-train: 100%|█████████████████████████| 591/591 [03:25<00:00,  2.88it/s]\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 17 lr: 0.00027\n",
      "train_average_losses_all_epoch:  0.06327003725254617\n",
      "train_losses_all_epoch:          0.03171246232208728\n",
      "train_losses_per_epoch:              0.013147856373290252\n",
      "val_average_losses_all_epoch:  0.031663522415658146\n",
      "val_losses_all_epoch:          0.024345160950824365\n",
      "val_losses_per_epoch:              0.01550228375620491\n",
      "acc_all_epoch:  99.37566137566138\n",
      "acc:            99.5952380952381\n",
      "11 epochs of increasing val loss\n",
      "Stopping training\n"
     ]
    },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  показывать аугментированные изображения и график в конце каждой эпохи\n",
    "show = False\n",
    "first = True\n",
    "\n",
    "# загружаем подель\n",
    "if model_filename == '':\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "    # Заменяем последний выходной слой\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, n_class)\n",
    "else:\n",
    "    model = torch.load(model_filename, map_location='cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimiser = optim.Adam(model.parameters(), \n",
    "                       lr=LR, \n",
    "                       betas=betas)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "sheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, \n",
    "                                                patience=2, \n",
    "                                                factor= 0.9)\n",
    "\n",
    "train_losses_all_epoch = [] # потери за все эпохи\n",
    "train_average_losses_all_epoch = [] # средние потери за все эпохи\n",
    "\n",
    "val_losses_all_epoch = [] # потери за все эпохи\n",
    "val_average_losses_all_epoch = [] # средние потери за все эпохи\n",
    "\n",
    "acc = 0\n",
    "acc_all_epoch = []\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "p = 0\n",
    "stop = False\n",
    "for epoch in range(EPOCH_COUNT):\n",
    "    model.train()\n",
    "    train_losses_per_epoch = []  # потери за текущую эпоху\n",
    "    for i, (train_images, train_labels) in enumerate(tqdm(train_loader, \n",
    "                                                          desc='epoch-'+str(epoch)+'-train',\n",
    "                                                          ncols=80,\n",
    "                                                          position=0)):\n",
    "        # Показать аугментированные изображения\n",
    "        if show or first:\n",
    "            first = False\n",
    "            imshow(train_images, train_labels, epoh=str(epoch), batch=str(i))\n",
    "        train_images = train_images.to(device=device)\n",
    "        train_labels = train_labels.to(device=device)\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(train_images)\n",
    "        loss = criterion(outputs, train_labels) # loss = error(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_losses_per_epoch.append(loss.item())\n",
    "        train_losses_all_epoch.append(np.mean(train_losses_per_epoch))\n",
    "\n",
    "    train_average_losses_all_epoch.append(np.mean(train_losses_all_epoch))\n",
    "\n",
    "    sheduler.step(np.mean(train_losses_per_epoch))\n",
    "\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    val_losses_per_epoch = []  # потери за текущую эпоху\n",
    "    with torch.no_grad():\n",
    "        for i, (val_images, val_labels) in enumerate(tqdm(test_loader,\n",
    "                                                          desc='epoch-'+str(epoch)+'-validate',\n",
    "                                                          ncols=80,\n",
    "                                                          leave=False,\n",
    "                                                          # position=1\n",
    "                                                          position=0\n",
    "                                                         )):\n",
    "            if show:\n",
    "                imshow(val_images, val_labels, epoh=str(epoch), batch=str(i))\n",
    "            count += len(val_labels)\n",
    "            val_images = val_images.to(device=device)\n",
    "            val_labels = val_labels.to(device=device)\n",
    "            outputs = model(val_images)\n",
    "            loss = criterion(outputs, val_labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == val_labels).sum().item( )\n",
    "            val_losses_per_epoch.append(loss.item())\n",
    "            val_losses_all_epoch.append(np.mean(val_losses_per_epoch))\n",
    "\n",
    "    val_average_losses_all_epoch.append(np.mean(val_losses_all_epoch))\n",
    "\n",
    "    acc = 100 * correct / float(count)\n",
    "    acc_all_epoch.append(acc)\n",
    "\n",
    "    val_loss = np.mean(val_losses_per_epoch)\n",
    "\n",
    "    # visualization loss\n",
    "    plt.cla()\n",
    "    plt.plot(train_average_losses_all_epoch, color=\"blue\")\n",
    "    plt.plot(val_average_losses_all_epoch, color=\"red\")\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"ResNet18: Loss vs Number of iteration\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig('resnet18_loss_e'+str(epoch)+'.png')\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"epoch: \" + str(epoch), \"lr: \" + str(get_lr(optimiser)))\n",
    "    print(\"train_average_losses_all_epoch: \", np.mean(train_average_losses_all_epoch))\n",
    "    print(\"train_losses_all_epoch:         \", np.mean(train_losses_all_epoch))\n",
    "    print(\"train_losses_per_epoch:             \", np.mean(train_losses_per_epoch))\n",
    "    print(\"val_average_losses_all_epoch: \", np.mean(val_average_losses_all_epoch))\n",
    "    print(\"val_losses_all_epoch:         \", np.mean(val_losses_all_epoch))\n",
    "    print(\"val_losses_per_epoch:             \", np.mean(val_losses_per_epoch))\n",
    "    print('acc_all_epoch: ', np.mean(acc_all_epoch))\n",
    "    print('acc:           ', acc)\n",
    "\n",
    "    # if epoch < 13:\n",
    "    #    valid_loss_min = np.Inf\n",
    "\n",
    "    if val_loss <= val_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                        val_loss_min,\n",
    "                        val_loss))\n",
    "        # torch.save(model, 'resnet18_e' + str(epoch) + '_' + '{:.6f}'.format(acc) + '_model.pth')\n",
    "        torch.save(model, 'model.pth')\n",
    "        val_loss_min = val_loss\n",
    "        p = 0\n",
    "\n",
    "    if val_loss > val_loss_min:\n",
    "        p += 1\n",
    "        print(f'{p} epochs of increasing val loss')\n",
    "        if p > patience:\n",
    "            print('Stopping training')\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "    if stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем модель на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "test_filename = os.path.join('/', 'kaggle', 'input', 'digit-recognizer', 'test.csv')\n",
    "model_filename = os.path.join('/', 'kaggle', 'working', 'model.pth')\n",
    "submission_filename = os.path.join('/', 'kaggle', 'working', 'submission.csv')\n",
    "\n",
    "# Используем GPU?\n",
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() and GPU else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# загружаем данные\n",
    "data = pd.read_csv(test_filename)\n",
    "\n",
    "# получаем изобрадения и переводим во float\n",
    "X_test = data.to_numpy()\n",
    "del data\n",
    "\n",
    "# визуализировать данные\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "for j in range(4*8):\n",
    "    ax = fig.add_subplot(4, 8, j+1)\n",
    "    i = random.randrange(0,len(X_test))\n",
    "    ax.imshow(X_test[i].reshape(28,28), cmap = cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()\n",
    "\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим в тензор\n",
    "X_test = torch.from_numpy(np.array(X_test, dtype=np.float32))\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = x.numpy()\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_GRAY2BGR) # Преобразование из 224,224,1 в 224,224,3\n",
    "            x = self.transform(image=x)\n",
    "            x = x['image']\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors.size(0)\n",
    "\n",
    "test_transform = albumentations.Compose([\n",
    "    albumentations.Resize(224, 224),\n",
    "    albumentations.pytorch.ToTensorV2()\n",
    "    ])\n",
    "\n",
    "test = CustomTensorDataset(X_test, transform=test_transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 875/875 [00:28<00:00, 31.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count:  28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_filename, map_location='cpu')\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "#num_ftrs = model.fc.in_features\n",
    "#model.fc = nn.Linear(num_ftrs, n_class)\n",
    "model = model.to(device)\n",
    "\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('ImageId,Label'+'\\r\\n')\n",
    "        for i, (val_images) in enumerate(tqdm(test_loader)):\n",
    "            val_images = val_images.to(device=device)\n",
    "            outputs = model(val_images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            for j, (p) in enumerate(predicted, 0):\n",
    "                out = \"{},{}\\r\\n\".format(count+j+1, p.item())\n",
    "                f.write(out)\n",
    "            count += len(predicted)\n",
    "        print('count: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0563379a97274356827019ece31f305f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2a165377be7541d0843198e5b49378c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e578677511649eeb8e2956d15d447d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e2c7103f36254470af48343fcdbbe140",
        "IPY_MODEL_9e213578de6c41ab8b875011607dc7f2"
       ],
       "layout": "IPY_MODEL_8798ec54476c4b8d9dbfeac12881118b"
      }
     },
     "8798ec54476c4b8d9dbfeac12881118b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e213578de6c41ab8b875011607dc7f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2a165377be7541d0843198e5b49378c3",
       "placeholder": "​",
       "style": "IPY_MODEL_0563379a97274356827019ece31f305f",
       "value": " 44.7M/44.7M [00:05&lt;00:00, 8.13MB/s]"
      }
     },
     "d5246bbc825940aa881f001486119097": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e2c7103f36254470af48343fcdbbe140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d5246bbc825940aa881f001486119097",
       "max": 46827520.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e5c50404b96e46bcbf7679691452e5b6",
       "value": 46827520.0
      }
     },
     "e5c50404b96e46bcbf7679691452e5b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
